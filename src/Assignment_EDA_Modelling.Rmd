
---
header-includes:
- \usepackage{xcolor}
- \usepackage{color} 
- \usepackage{fancyhdr,color}
- \usepackage{lipsum}
- \fancyfoot[CE] {\thepage}
title: "Assignment 3"
subtitle: "Applied Forecasting in Complex Systems 2025"
author: Bugra Sipahioglu (14318334), Despoina Delipalla (16324897), Jeremi Wasilewski (14271028)
date: "University of Amsterdam \n &nbsp;  \n December 23, 2025"
output: pdf_document
fontsize: 11pt
highlight: tango
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  fig.align = 'center',   # center plots
  #fig.width = 5,          # width in inches (smaller than default 6)
  #fig.height = 2,         # height in inches (smaller than default 4)
  fig.pos = 'H',          # keep figures in place
  echo = TRUE,
  message = FALSE,
  warning = FALSE,
  cache = TRUE,
  dev.args = list(pointsize = 11)
)
options(digits = 3, width = 60)

#install.packages("readr")
#install.packages("tsibble")

library(readr)
library(dplyr)
library(lubridate)
library(dplyr)
library(tidyr)
library(readr)
library(ggplot2)
library(tsibble)
library(ggplot2)
library(fpp3)
library(fable)
library(feasts)
```

# Exploratory Data Analysis

#### Load the Data

```{r}
calendar <- read_csv("../data/calendar_afcs2025.csv")  # date, event_type, week_id
sell_prices <- read_csv("../data/sell_prices_afcs2025.csv")  # week_id, item_id, price
sales_data  <- read_csv("../data/sales_train_validation_afcs2025.csv")  # item_id, product_type, items_sold
     
```

#### Calendar

```{r}
# Transform date format to date and ensure that dates are in chronological order
calendar <- calendar %>%
  mutate(date = as.Date(date, format = "%m/%d/%Y"))%>%
 arrange(date)


# Add column day_id
calendar <- calendar %>%   
  mutate(day_id = paste0("id_", row_number()))
```

```{r}
# rows and columns
no_of_rows = nrow(calendar)
no_of_cols = ncol(calendar)
col_names = colnames(calendar)

# Dates
range = range(calendar$date)

# NA values
rows_na = colSums(is.na(calendar))

# Duplicates
dupls = anyDuplicated(calendar)

no_of_rows
no_of_cols
col_names
range
rows_na
dupls

```

Colendard contains information about the dates the products are sold.
Calendar contains columns:

-   date: The date in a "y-m-d" format.

-   wm_yr_wk: The id of the week the date belongs to

-   weekday: The type of the day (Saturday, Sunday, …, Friday).

-   wday: The id of the weekday, starting from Saturday.

-   month: The month of the date.

-   year: The year of the date.

-   event_name_1: If the date includes an event, the name of this event.

-   event_type_1: If the date includes an event, the type of this event.

-   event_name_2: If the date includes a second event, the name of this
    event.

-   event_type_2: If the date includes a second event, the type of this
    event.

-   snap_TX: A binary variable (0 or 1) indicating whether the stores of
    TX, allow SNAP3 purchases on the examined date. 1 indicates that
    SNAP purchases are allowed.

The dataset covers the period from 29/01/2011 to 19/06/2011. There are
1807 NA values in the event_type_1 columns and 1964 NA values in the
event_type_2 column, indicating that there were no events on the
corresponding dates.

#### Sell Prices

```{r}
# rows and columns
no_of_rows = nrow(sell_prices)
no_of_cols = ncol(sell_prices)
col_names = colnames(sell_prices)

# NA values
rows_na = colSums(is.na(sell_prices))

# Duplicates
dupls = anyDuplicated(sell_prices)

no_of_rows 
no_of_cols
col_names 
rows_na 
dupls 
```

#### Sales

split column id to store_id and item_id, so they match with the
corresponding columns from sell_prices and calendar: item_id: item_id,
dept_id, cat_id store_id: state_id, dtate_store_id

Convert dataset to long format. Create column named day_id that matches
with calendar's day_id and column units sold that contains information
about the number of units sold per day.

```{r}
sales_nc <- sales_data %>%
  separate(
    id,
    into = c("item_id", "dept_id", "cat_id", "state_id", "state_store_id", "dataset_name"),
    sep = "_"
  )%>%
  mutate(
    store_id = paste(state_id, state_store_id, sep = "_"),
    item_id = paste(item_id, dept_id, cat_id, sep = "_")  
  ) %>%
  select(-state_id, -state_store_id)
sales <- sales_nc %>%
  pivot_longer(
    cols = starts_with("d_"),
    names_to  = "day_id",
    names_transform = list(day_id = ~ sub("^d_", "id_", .)),
    values_to = "units_sold"
  )
```

```{r}
# rows and columns
no_of_rows = nrow(sales)
no_of_cols = ncol(sales)
col_names = colnames(sales)

# NA values
rows_na = colSums(is.na(sales))

# Duplicates
dupls = anyDuplicated(sales)

no_of_rows
no_of_cols
col_names
rows_na
dupls
summary(sales["units_sold"])
```

The dataset has 1574399 rows. There are no missing values.\
The mean number of units sold was 1.904.

#### **Join calendar and sales information**

A left join keeps all rows from the left table and adds matching columns
from the right table. In this case we use this type of join, because we
want keep all information from sales, but not information from calendar
that are actually useless

```{r}
calendar_sales <- sales%>%
  left_join(calendar, by = "day_id")

head(calendar_sales)
```

#### **Join weekly prices and calendar_sales**

```{r}
full_dataset <- calendar_sales %>%
  left_join(
    sell_prices,
    by = c("store_id", "item_id", "wm_yr_wk")
  )
```

#### **Data Exploration**

Data types.

```{r}
str(full_dataset)
```

```{r}
no_of_rows = nrow(full_dataset)
no_of_cols = ncol(full_dataset)
col_names = colnames(full_dataset)

# NA values
rows_na = colSums(is.na(full_dataset))

# Duplicates
dupls = anyDuplicated(full_dataset)

no_of_rows
no_of_cols
col_names
rows_na
dupls
```

```{r}

# Proportions of missing values for event 1 and event 2 in calendar dataset vs full dataset

mean(is.na(calendar$event_name_1))
mean(is.na(full_dataset$event_name_1))

mean(is.na(calendar$event_name_2))
mean(is.na(full_dataset$event_name_2))
```

```{r}
# Proportion of missing prices at daily level
mean(is.na(full_dataset$sell_price))

# Proportion of missing prices at weekly level
mean(
  is.na(
    full_dataset %>%
      distinct(store_id, item_id, wm_yr_wk, sell_price) %>%
      pull(sell_price)
  )
)

```

The dataset containing all information consists of 1574399 rows, which
is identical to the number of rows in the sales dataset.\
The dataset has 1447657 and 1571106 missing values for event type 1 and
2, respectively. These number are larger than the corresponding numbers
in dataset calendar, however the proportion of missing values in the
datasets match, indicating that the missing event information is
expected and correctly propagated through the join.\
The missing sell_price values occur on weeks where no price was recorded
(e.g., the item was not sold or not available), and these weekly gaps
are repeated across daily observations after joining prices to the sales
data. This is confirmed by the fact that the proportion of missing
values on a daily basis is similar to the proportion of missing values
on a weekly basis.

Zero sales and missing sales

```{r}

full_dataset %>%
  summarise(
    zero_sales = sum(units_sold == 0, na.rm = TRUE),
    na_sales   = sum(is.na(units_sold))
  )
```

Date Continuity

```{r}
full_dataset %>%
  arrange(store_id, item_id, date) %>%
  group_by(store_id, item_id) %>%
  summarise(
    min_date = min(date),
    max_date = max(date),
    n_days   = n(), #number of rows in data 
    span     = as.integer(max_date - min_date) + 1, # number of days with data
    .groups = "drop"
  ) %>%
  filter(n_days != span)
```

There are no missing dates

Summary

```{r}
summary(full_dataset$units_sold)
summary(full_dataset$sell_price)
```

Price per date

```{r}
full_dataset %>%
  filter(!is.na(sell_price)) %>%
  group_by(date) %>%
  summarise(avg_price = mean(sell_price)) %>%
  as_tsibble(index = date) %>%
  autoplot(avg_price)
```

Price - Event

```{r}
full_dataset%>%
  filter(!is.na(sell_price)) %>%
  mutate(event = if_else(is.na(event_name_1), "No event", "Event")) %>%
  group_by(date, event) %>%
  summarise(avg_price = mean(sell_price), .groups = "drop") %>%
  as_tsibble(key = event, index = date) %>%
  autoplot(avg_price)
```

Sales per weekday

```{r}
full_dataset%>%
  group_by(weekday) %>%
  summarise(avg_sales = mean(units_sold)) %>%
  ggplot(aes(weekday, avg_sales)) +
  geom_col()
```

Sales per category

```{r}
full_dataset%>%
  group_by(cat_id) %>%
  summarise(avg_sales = mean(units_sold)) %>%
  ggplot(aes(cat_id, avg_sales)) +
  geom_col()
```

Event Impact

```{r}
full_dataset%>%
  mutate(event = if_else(is.na(event_name_1), "No event", "Event")) %>%
  group_by(date, event) %>%
  summarise(total_sales = sum(units_sold), .groups = "drop") %>%
  as_tsibble(key = event, index = date) %>%
  autoplot(total_sales)
```

SNAP effect

```{r}
 full_dataset%>%
  group_by(date, snap_TX) %>%
  summarise(total_sales = sum(units_sold), .groups = "drop") %>%
  as_tsibble(key = snap_TX, index = date) %>%
  autoplot(total_sales)
```

#### Total Sales

```{r}
full_dataset%>%
  group_by(date) %>%
  summarise(total_sales = sum(units_sold)) %>%
  as_tsibble(index = date) %>%
  autoplot(total_sales)
```

Weekly Sales

```{r}
full_dataset %>%
  group_by(week = yearweek(date)) %>%
  summarise(total_sales = sum(units_sold), .groups = "drop") %>%
  as_tsibble(index = week) %>%
  autoplot(total_sales)
```

STL Decomposition

```{r}
full_dataset %>%
  group_by(week = yearweek(date)) %>%
  summarise(total_sales = sum(units_sold), .groups = "drop") %>%
  as_tsibble(index = week) %>%
  model(STL(total_sales)) %>%
  components() %>%
  autoplot() +
  labs(
    title = "STL Decomposition of Weekly Total Sales",
    x = "Week",
    y = "Units Sold"
  )
```

Increasing trend\
Seasonality (same pattern every year)

```{r}
full_dataset %>%
  group_by(date) %>%
  summarise(total_sales = sum(units_sold), .groups = "drop") %>%
  as_tsibble(index = date) %>%
  model(
    STL(total_sales ~ season(window = "periodic"))
  ) %>%
  components() %>%
  autoplot() +
  labs(
    title = "STL Decomposition of Daily Total Sales",
    x = "Date",
    y = "Units Sold"
  )
```

# Modelling

## Seasonal Naive Model

### 1. Pre-processing

```{r}
# Put all the data into daily time-series format
texas_daily <- full_dataset %>%
  mutate(date = as.Date(date)) %>%
  group_by(date) %>%
  summarise(sales = sum(units_sold, na.rm = TRUE), .groups = "drop") %>%
  arrange(date) %>%
  as_tsibble(index = date)

# Determine the horizon and thus the cutoff value
forecast_horizon <- 28
cutoff <- max(texas_daily$date) - days(forecast_horizon)

# Split data into train and validation
ts_train <- texas_daily %>% filter(date <= cutoff)
ts_validation  <- texas_daily %>% filter(date > cutoff)
```

### 2. Fit the model

Fit the seasonal naive model for weekly lag and check the other
baselines for sanity check since the sales data is seasonal, and
seasonal naive should produce the lowest RMSE. We'll use seasonal naive
in the report as a baseline.

```{r}
# Fit the baseline models
models <- ts_train %>%
  model(
    Mean          = MEAN(sales),
    Naive         = NAIVE(sales),
    Seasonal_naive= SNAIVE(sales ~ lag("week")),
    Drift         = RW(sales ~ drift())
  )
```

### 3. Forecast and report the results

```{r}
# Forecast and report 80% & 90% prediction intervals
fc_baseline <- models %>% forecast(h = forecast_horizon, level = c(80, 95))

# Calculate accuracy for the validation predictions
accuracy_baseline <- accuracy(fc_baseline, ts_validation)
accuracy_baseline

# Plot the data and predictions
autoplot(ts_train, sales) +
  autolayer(fc_baseline) +
  labs(title = "Baseline Forecasts (28 days)", x = "Date", y = "Sales")
```

### 5. Diagnostics (if needed but I don't think so since these are baselines)

```{r}

models |>
  select(Mean) |>
  gg_tsresiduals() +
  labs(title = "Residual diagnostics: Mean")

models |>
  select(Naive) |>
  gg_tsresiduals() +
  labs(title = "Residual diagnostics: Naive")

models |>
  select(Seasonal_naive) |>
  gg_tsresiduals() +
  labs(title = "Residual diagnostics: Seasonal naive")

models |>
  select(Drift) |>
  gg_tsresiduals() +
  labs(title = "Residual diagnostics: Drift")
```

## ARIMA

### 1. Pre-processing

No fundamental pre-processing is needed as the dataset that's used in
seasonal naive (i.e., texas_daily) is sufficient for ARIMA modelling.

```{r}
# No fundamental preprocessing is needed compared to dataset that's used
```

Decide whether a (log) transformation is needed or not.

```{r}
# Plot the sales data
autoplot(ts_train, sales)
```

Since the variance is relatively stable across all levels, no
transformation is needed here.

### 2. Fit the model and report the (automatically) chosen model

```{r}
fit_arima <- ts_train %>%
  model(arima = ARIMA(sales))

# report the chosen model
report(fit_arima)
```

### 3. Forecast and report the results

```{r}
# Forecast and report 80% & 90% prediction intervals
fc_arima <- fit_arima %>%
  forecast(h = forecast_horizon, level = c(80, 95))


# Calculate accuracy for the validation predictions
accuracy_arima <- accuracy(fc_arima, ts_validation)
accuracy_arima

# Plot the predictions
autoplot(ts_train, sales) +
  autolayer(fc_arima) +
  labs(title = "ARIMA Forecast (28 days)", x = "Date", y = "Sales")
```

### 4. Residual Diagnostics

```{r}
fit_arima |>
  gg_tsresiduals()

arima_resid <- fit_arima %>%
  augment()
# Residual Time Plot
arima_resid %>%
  ggplot(aes(x = date, y = .resid)) +
  geom_line() +
  geom_hline(yintercept = 0, color = "red") +
  labs(title = "ARIMA Residuals Over Time",
       x = "Date", y = "Residual")

# Residual ACF
arima_resid %>%
  ACF(.resid) %>%
  autoplot() +
  labs(title = "ACF of ARIMA Residuals")

# Residual PACF
arima_resid %>%
  PACF(.resid) %>%
  autoplot() +
  labs(title = "PACF of ARIMA Residuals")

# Ljung - Box
arima_resid %>%
  features(.resid, ljung_box, lag = 28, dof = 0)
# Histogram
arima_resid %>%
  ggplot(aes(x = .resid)) +
  geom_histogram(bins = 30, fill = "gray", color = "black") +
  labs(title = "Histogram of ARIMA Residuals",
       x = "Residual", y = "Count")

```

```{r}
arima_resid <- fit_arima %>%
  augment()
# Residual Time Plot
arima_resid %>%
  ggplot(aes(x = date, y = .resid)) +
  geom_line() +
  geom_hline(yintercept = 0, color = "red") +
  labs(title = "ARIMA Residuals Over Time",
       x = "Date", y = "Residual")

# Residual ACF
arima_resid %>%
  ACF(.resid) %>%
  autoplot() +
  labs(title = "ACF of ARIMA Residuals")

# Residual PACF
arima_resid %>%
  PACF(.resid) %>%
  autoplot() +
  labs(title = "PACF of ARIMA Residuals")

# Ljung - Box
arima_resid %>%
  features(.resid, ljung_box, lag = 28, dof = 0)
# Histogram
arima_resid %>%
  ggplot(aes(x = .resid)) +
  geom_histogram(bins = 30, fill = "gray", color = "black") +
  labs(title = "Histogram of ARIMA Residuals",
       x = "Residual", y = "Count")
```

## ETS model

### 1. Pre-processing

No prepcoessing to the data is necessary compared to previous models

### 1. Fit the model
```{r}
fit_ets <- ts_train %>%
model(ETS = ETS(sales))

report(fit_ets)

```


### 2. Generate and plot the predictions


```{r}
# generate the predcitions
fc_ets <- fit_ets %>%
forecast(h = forecast_horizon, level = c(80, 95))

# plot the predictions
autoplot(ts_train, sales) +
autolayer(fc_ets) +
labs(title = "ETS Forecast (28 days)", x = "Date", y = "Sales")

# get the accuracy
accuracy_ets <- accuracy(fc_ets, ts_validation)
accuracy_ets
```


### 3. Resdual diagnostics

```{r}
fit_ets |>
  gg_tsresiduals()
```


## LightGBM model

### 1. Pre-processsing

In order to create features for light gbm model - we need to prepare full_dataset to handle the feature enginerreing from 'LightGBM.Rmd' notebook

```{r}
library(dplyr)
library(data.table)
library(lightgbm)

# parse full_dataset as table
dt <- as.data.table(full_dataset)

# rename units_sold to sales to match Chris' naming
dt[, sales := as.numeric(units_sold)]
# ensure date is in date format
dt[, date := as.Date(date)]

# create uniqe series id (one time series per item-store)
dt[, id := paste(item_id, store_id, sep = "_")]

# sort for correct lags/rolling
setorder(dt, id, date)  # [web:307]

# forward-fill prices within each series
dt[, sell_price_ff := nafill(sell_price, type = "locf"), by = id]  # [web:410]

# lags
for (L in c(1, 7, 14, 28)) {
  dt[, paste0("lag_", L) := shift(sales, n = L, type = "lag"), by = id]  # [web:137]
}

# rolling statistics
dt[, roll_mean_7  := frollmean(sales, 7,  align = "right"), by = id]
dt[, roll_mean_28 := frollmean(sales, 28, align = "right"), by = id]
dt[, roll_sd_28   := frollapply(sales, 28, sd, align = "right"), by = id]

# intermittency
dt[, nz_28 := frollsum(as.integer(sales > 0), 28, align = "right"), by = id]
dt[, zero_ratio_28 := 1 - nz_28 / 28]

# price features
dt[, price_lag_7 := shift(sell_price_ff, 7), by = id]
dt[, price_pct_change_7 :=
      fifelse(price_lag_7 > 0,
              (sell_price_ff - price_lag_7) / price_lag_7,
              0.0)]
dt[, is_price_drop := as.integer(price_pct_change_7 < 0)]
dt[, price_mean_item := mean(sell_price_ff, na.rm = TRUE), by = id]
dt[, price_rel := sell_price_ff / price_mean_item]

# time-validation
max_date  <- max(dt$date, na.rm = TRUE)
val_start <- max_date - 27

# tomorrow target
dt[, target_1 := shift(sales, 1, type = "lead"), by = id]  

train_1 <- dt[date <  val_start & !is.na(target_1)]
val_1   <- dt[date >= val_start & !is.na(target_1)]
```


Prepare features:

* wday: Day of the week
* month: Month
* year: Year
* snap_TX: Binary indicator for SNAP eligibility in Texas
* lag_1: Sales from yesterday
* lag_7: Sales from last week
* lag_28: Sales from last month
* roll_mean_7: Average sales over last 7 days
* roll_mean_28: Average sales over last month
* roll_sd_28: Standard deviation of sales over last month
* zero_ratio_28: Fraction of zero-sales days in last 28 days (Distinguishes slow-moving items from consistently selling ones)
* sell_price_ff: Current effective price
* price_rel: Current price relative to item’s average price (Is the item expensive or discounted relative to itself)
* price_pct_change_7: Percentage price change over the last 7 days
* is_price_drop: Binary indicator of a recent price decrease.

```{r}
feat_cols <- c(
  "wday","month","year","snap_TX",
  "lag_1","lag_7","lag_14","lag_28",
  "roll_mean_7","roll_mean_28","roll_sd_28","zero_ratio_28",
  "sell_price_ff","price_rel","price_pct_change_7","is_price_drop"
)


missing_feats <- setdiff(feat_cols, names(dt))
print(missing_feats)
stopifnot(length(missing_feats) == 0)

train_1 <- train_1[complete.cases(train_1[, ..feat_cols])]
val_1   <- val_1[complete.cases(val_1[, ..feat_cols])]

```

```{r}
y_tr <- log1p(train_1$target_1)
y_va <- log1p(val_1$target_1)

X_tr <- as.matrix(train_1[, ..feat_cols])
X_va <- as.matrix(val_1[, ..feat_cols])

```


### 2. Fit the model

```{r}

dtrain <- lgb.Dataset(X_tr, label = y_tr)
dval   <- lgb.Dataset(X_va, label = y_va)

params <- list(
  objective = "regression",
  metric = "rmse",
  learning_rate = 0.05,
  num_leaves = 64,
  feature_fraction = 0.8,
  bagging_fraction = 0.8,
  bagging_freq = 1,
  min_data_in_leaf = 50
)

m1 <- lgb.train(
  params = params,
  data = dtrain,
  nrounds = 5000,
  valids = list(val = dval),
  early_stopping_rounds = 100,
  verbose = 1
)

```

### 3. Validate the predictions

```{r}
# prediction for target values
pred <- expm1(predict(m1, X_va))

# factual target values
y <- val_1$target_1

# residuals
e <- y - pred

# calcualte accuracy metrics
me_lgbm   <- mean(e, na.rm = TRUE)
rmse_lgbm <- sqrt(mean(e^2, na.rm = TRUE))
mae_lgbm  <- mean(abs(e), na.rm = TRUE)

# calculate autocorrelation for lag 1
acf1_lgbm <- as.numeric(stats::acf(e[is.finite(e)], plot = FALSE, lag.max = 1)$acf[2])  

# parse metrics as table
accuracy_lgbm <- data.frame(
  .model = "LightGBM",
  .type  = "Test",
  ME = me_lgbm,
  RMSE = rmse_lgbm,
  MAE = mae_lgbm,
  ACF1 = acf1_lgbm
)

# present metrics
accuracy_lgbm


```

### 3. Evaluate residuals
```{r}

library(patchwork)

# get predictions for train data
fitted_train_values <- expm1(predict(m1, X_tr))

# actual data
actual_train_values <- train_1$target_1  

# calculate residuals
train_residuals <- actual_train_values - fitted_train_values

# attach residuals to the training table and shift to the day being predicted
train_dt <- as.data.table(train_1)
train_dt[, train_residual := train_residuals]

# target_1 is sales on the next day, so residuals are date + 1
train_dt[, predicted_date := date + 1L]

# for each timestamp take an averagre of residuals 
daily_train_residuals <- train_dt[
  , .(average_residual = mean(train_residual, na.rm = TRUE)),
  by = predicted_date
]

# save in the tsibble format to use acf and autplot
daily_train_residual_ts <- as_tsibble(
  daily_train_residuals,
  index = predicted_date
)

# plot residuals over time
p_time_tr <- autoplot(daily_train_residual_ts, average_residual) +
  labs(
    title = "LightGBM fit residuals",
    x = "date",
    y = "Innovation residuals"
  )

# plot autocorrelation 
p_acf_tr <- daily_train_residual_ts |>
  ACF(average_residual) |>
  autoplot() +
  labs(title = NULL, x = "lag [1D]", y = "acf")

# plot histogram of residuals
p_hist_tr <- ggplot(daily_train_residuals, aes(x = average_residual)) +
  geom_histogram(bins = 50) +
  geom_rug(sides = "b", alpha = 0.25) +
  labs(title = NULL, x = ".resid", y = "count")

# present plots
p_time_tr / (p_acf_tr | p_hist_tr)

```

### 4. Tune hyperparameters

```{r}
# insipration for approach: https://github.com/microsoft/LightGBM/issues/4642

dtrain <- lgb.Dataset(
  data  = X_tr,
  label = y_tr,
  params = list(feature_pre_filter = FALSE)  # fixed here
)

# parameters that stay fixed
base_params <- list(
  objective = "regression",
  metric    = "rmse"
)

# grid with different tuning parameters
param_grid <- expand.grid(
  learning_rate    = c(0.03, 0.05),
  num_leaves       = c(15, 31, 64),
  feature_fraction = c(0.7, 0.8, 0.9),
  bagging_fraction = c(0.7, 0.8, 0.9),
  min_data_in_leaf = c(10, 20, 50),
  KEEP.OUT.ATTRS = FALSE
)

#.set seed for reproduction
set.seed(123)

```


Run cross validation.
Note: At the moment commented out for knitting efficiency. The output of the cross validaiton is saved in csv file and read in the code at the bottom of the next chunk.

```{r} eval=FALSE
# cv settings
# for computing efficiency only 10 rounds of cross-validation are used per each parameter setting
nrounds_cv      <- 100
early_stopping  <- 10
n_folds         <- 3

# store rmse for each cross validation
cv_scores <- numeric(nrow(param_grid))

# loop over all hyperparameter combinations in the grid.
for (i in seq_len(nrow(param_grid))) {
  
  # get the parameters
  params_i <- c(base_params, as.list(param_grid[i, ]))
  
  # Run k-fold cross-validation for this parameter setting
  cv <- lgb.cv(
    params = params_i,
    data   = dtrain,
    nrounds = nrounds_cv,
    nfold   = n_folds,
    early_stopping_rounds = early_stopping,
    verbose = -1
  )
  
  # store the min RMSE value 
  cv_scores[i] <- min(unlist(cv$record_evals$valid$rmse$eval))
}

# find the index of lowest rmse
best_idx   <- which.min(cv_scores)

# get the parameters for that list
best_param <- c(base_params, as.list(param_grid[best_idx, ]))
best_param
write.csv(best_param_df, "best_lightgbm_params.csv", row.names = FALSE)

```

Read identified tuned hyperparemeters

```{r}
best_param_df <- read.csv("best_lightgbm_params.csv", stringsAsFactors = FALSE)
best_param_df

```

The best model is identified as follows
- learning rate: 0.05
- num leaves: 64
- feature fraction: 0.9
- bagging fraction: 0.9
- min_data_in_leaf: 20

Let's test the model with such settings on validation test

### 5. Validate the predcitions and see if predictions improved and analyse the residuals

```{r}

# train tuned model on training data, validate on val_1
dtrain_tuned <- lgb.Dataset(X_tr, label = y_tr)
dval_tuned   <- lgb.Dataset(X_va, label = y_va)

m1_tuned <- lgb.train(
  params  = best_param,    # parameters from grid search
  data    = dtrain_tuned,
  nrounds = 5000,         
  valids  = list(val = dval_tuned),
  early_stopping_rounds = 100,
  verbose = 1
)

# Predictions and accuracy on validation set
pred_tuned <- expm1(predict(m1_tuned, X_va))
y_val      <- val_1$target_1

resid_tuned <- y_val - pred_tuned

# calcualte metrics
me_lgbm_tuned   <- mean(resid_tuned, na.rm = TRUE)
rmse_lgbm_tuned <- sqrt(mean(resid_tuned^2, na.rm = TRUE))
mae_lgbm_tuned  <- mean(abs(resid_tuned), na.rm = TRUE)
acf1_lgbm_tuned <- as.numeric(
  stats::acf(resid_tuned[is.finite(resid_tuned)], plot = FALSE, lag.max = 1)$acf[2]
)

# summarise metrics in the table
accuracy_lgbm_tuned <- data.frame(
  .model = "LightGBM_tuned",
  .type  = "Test",
  ME     = me_lgbm_tuned,
  RMSE   = rmse_lgbm_tuned,
  MAE    = mae_lgbm_tuned,
  ACF1   = acf1_lgbm_tuned
)

# print metrics
accuracy_lgbm_tuned

```
Investigate residuals in an analogical way to m1 model
```{r}

# Investigate residuals for tuned m1 analogically to m1


fitted_train_values_tuned <- expm1(predict(m1_tuned, X_tr))
actual_train_values       <- train_1$target_1
train_residuals_tuned     <- actual_train_values - fitted_train_values_tuned


train_dt_tuned <- as.data.table(train_1)
train_dt_tuned[, train_residual_tuned := train_residuals_tuned]
train_dt_tuned[, predicted_date := date + 1L]


daily_train_residuals_tuned <- train_dt_tuned[
  , .(average_residual_tuned = mean(train_residual_tuned, na.rm = TRUE)),
  by = predicted_date
]

daily_train_residual_ts_tuned <- as_tsibble(
  daily_train_residuals_tuned,
  index = predicted_date
)

# residuals over time
p_time_tr_tuned <- autoplot(daily_train_residual_ts_tuned, average_residual_tuned) +
  labs(
    title = "LightGBM tuned fit residuals",
    x = "date",
    y = "Innovation residuals"
  )

# autocorrelation
p_acf_tr_tuned <- daily_train_residual_ts_tuned |>
  ACF(average_residual_tuned) |>
  autoplot() +
  labs(title = NULL, x = "lag [1D]", y = "acf")

# histogram
p_hist_tr_tuned <- ggplot(daily_train_residuals_tuned, aes(x = average_residual_tuned)) +
  geom_histogram(bins = 50) +
  geom_rug(sides = "b", alpha = 0.25) +
  labs(title = NULL, x = ".resid", y = "count")

p_time_tr_tuned / (p_acf_tr_tuned | p_hist_tr_tuned)

```


## Generate table with results of all models

```{r}
allresults <- bind_rows(
  accuracy_baseline,
  accuracy_arima,
  accuracy_ets,
  accuracy_lgbm,
  accuracy_lgbm_tuned
)

allresults

```

